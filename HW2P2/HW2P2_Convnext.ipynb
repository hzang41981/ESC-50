{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hzang41981/ESC-50/blob/master/HW2P2/HW2P2_Convnext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "KOquhcku23HF",
        "outputId": "5b6cd257-0c93-4a35-d4c8-c0f6f7d03981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "/content\n",
            "/content/drive\n",
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jwLEd0gdPbSc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOs\n",
        "As you go, please read the code and keep an eye out for TODOs!"
      ],
      "metadata": {
        "id": "1oxQNl-YVWHc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BksgPdkQwwb",
        "outputId": "08519e18-2f85-4211-957c-05af9e96c50f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 2.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=8736accb10ae99c39b8e10620ae0c4bc76e391e07ae3fdda1e8574683998a48c\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"hzang41981\",\"key\":\"07ce2b8c32b2a27182316ae7fe638b80\"}') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oFjaJTaRjT7",
        "outputId": "cf9b9c5b-365a-49f6-bc32-e072bb73d596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s22-hw2p2-classification.zip to /content\n",
            "100% 2.35G/2.35G [00:32<00:00, 122MB/s]\n",
            "100% 2.35G/2.35G [00:32<00:00, 78.7MB/s]\n",
            "Downloading 11-785-s22-hw2p2-verification.zip to /content\n",
            " 96% 252M/263M [00:01<00:00, 198MB/s]\n",
            "100% 263M/263M [00:01<00:00, 176MB/s]\n",
            "11-785-s22-hw2p2-classification.zip   sample_data\n",
            "11-785-s22-hw2p2-verification.zip     train_subset\n",
            "classification\t\t\t      verification\n",
            "classification_sample_submission.csv  verification_sample_submission.csv\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "!unzip -q 11-785-s22-hw2p2-classification.zip\n",
        "!unzip -q 11-785-s22-hw2p2-verification.zip\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTLCyocZBGS"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "13usn4nYZCvJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "\"\"\"\n",
        "batch_size = 256\n",
        "lr = 0.1\n",
        "epochs = 20 # Just for the early submission. We'd want you to train like 50 epochs for your main submissions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from audioop import bias\n",
        "from tokenize import group\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 expand_ratio):\n",
        "        super().__init__() # Just have to do this for all nn.Module classes\n",
        "\n",
        "        # Can only do identity residual connection if input & output are the\n",
        "        # same channel & spatial shape.\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "        \n",
        "        # Expand Ratio is like 6, so hidden_dim >> in_channels\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(in_channels,in_channels,kernel_size=7,padding=3,groups=in_channels,bias=False),\n",
        "            nn.BatchNorm2d(in_channels)\n",
        "        )\n",
        "\n",
        "       \n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(in_channels,hidden_dim,kernel_size=1,stride=1,padding=0,bias=False),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(hidden_dim,out_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.spatial_mixing(x)\n",
        "        out = self.feature_mixing(out)\n",
        "        out= self.bottleneck_channels(out)\n",
        "\n",
        "        if self.do_identity:\n",
        "            return x + out\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes= 7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        \"\"\"\n",
        "        First couple of layers are special, just do them here.\n",
        "        This is called the \"stem\". Usually, methods use it to downsample or twice.\n",
        "        \"\"\"\n",
        "\n",
        "        self.stem = nn.ModuleList()\n",
        "        stem = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            nn.Conv2d(3,96,kernel_size=4,stride=4,bias=False),\n",
        "            nn.BatchNorm2d(96)\n",
        "        )\n",
        "        self.stem.append(stem)\n",
        "\n",
        "        #down_sample\n",
        "        res1=nn.Sequential(\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96,192,kernel_size=2,stride=2,bias=False),)\n",
        "        self.stem.append(res1)\n",
        "        \n",
        "        res2=nn.Sequential(\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.Conv2d(192,384,kernel_size=2,stride=2,bias=False),)\n",
        "        self.stem.append(res2)\n",
        "\n",
        "        res3=nn.Sequential(\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.Conv2d(384,768,kernel_size=2,stride=2,bias=False),)\n",
        "        self.stem.append(res3)\n",
        "        \n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [4,  96, 3, 1],\n",
        "            [4,  192, 3, 1],\n",
        "            [4,  384, 9, 1],\n",
        "            [4,  768, 3, 1],\n",
        "        ]\n",
        "\n",
        "        # Remember that our stem left us off at 16 channels. We're going to \n",
        "        # keep updating this in_channels variable as we go\n",
        "        in_channels = 96\n",
        "\n",
        "        # Let's make the layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for curr_stage in self.stage_cfgs:\n",
        "            expand_ratio, num_channels, num_blocks, stride = curr_stage\n",
        "            layers = []\n",
        "            for block_idx in range(num_blocks):\n",
        "                in_channels=num_channels\n",
        "                out_channels = num_channels\n",
        "                layers.append(InvertedResidualBlock(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    # only have non-trivial stride if first block\n",
        "                    stride=stride if block_idx == 0 else 1, \n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "                # In channels of the next block is the out_channels of the current one\n",
        "                # in_channels = out_channels \n",
        "            layers = nn.Sequential(*layers)\n",
        "            self.layers.append(layers) \n",
        "         # Done, save them to the class\n",
        "\n",
        "        # Some final feature mixing\n",
        "        self.final_block = nn.Sequential(\n",
        "            nn.BatchNorm2d(768)\n",
        "        )\n",
        "\n",
        "        # Now, we need to build the final classification layer.\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            # TODO: Fill this in!\n",
        "            # Pool over & collapse the spatial dimensions to (1, 1)\n",
        "            # Collapse the trivial (1, 1) dimensions\n",
        "            # Project to our # of classes\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.BatchNorm2d(768),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(768,num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"\n",
        "        Usually, I like to use default pytorch initialization for stuff, but\n",
        "        MobileNetV2 made a point of putting in some custom ones, so let's just\n",
        "        use them.\n",
        "        \"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        out=x\n",
        "        for i in range(4):\n",
        "            # print(i)\n",
        "            \n",
        "            out=self.stem[i](out)\n",
        "            # print(out.shape)\n",
        "            out=self.layers[i](out)\n",
        "        # out = self.final_block(out)\n",
        "        out = self.cls_layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "csLDrvKnkFBw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Very Simple Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwYR-CLwX09u"
      },
      "source": [
        "# Dataset & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awE5BxlqX2o7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Transforms (data augmentation) is quite important for this task.\n",
        "Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "\"\"\"\n",
        "DATA_DIR = \"/content\"\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") # This is a smaller subset of the data. Should change this to classification/classification/train\n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "\n",
        "train_transforms = [ttf.ToTensor()]\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform=ttf.Compose(train_transforms))\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform=ttf.Compose(val_transforms))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CTRGtI6yA4KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UowI9OcUYPjP",
        "outputId": "71d9cc98-9fe2-4d26-de10-b4950b12c40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params: 33158200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-01fa235276ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "model = ConvNeXt()\n",
        "#model.cuda()\n",
        "\n",
        "# For this homework, we're limiting you to 35 million trainable parameters, as\n",
        "# outputted by this. This is to help constrain your search space and maintain\n",
        "# reasonable training times & expectations\n",
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "# TODO: What criterion do we use for this task?\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n",
        "\n",
        "# For this homework, we strongly strongly recommend using FP16 to speed up training.\n",
        "# It helps more for larger models.\n",
        "# Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n",
        "# and compare \"Single precision training\" section with \"Mixed precision training\" section\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ConvNeXt())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OblDjOHDzNef",
        "outputId": "41be60f4-7f3e-433a-f27c-dbbe4f5bc0da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNeXt(\n",
            "  (stem): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), bias=False)\n",
            "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
            "    )\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (1): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (2): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (1): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (2): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(192, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (1): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (2): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (3): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (4): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (5): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (6): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (7): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (8): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(1536, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (1): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (2): InvertedResidualBlock(\n",
            "        (spatial_mixing): Sequential(\n",
            "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (feature_mixing): Sequential(\n",
            "          (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): GELU()\n",
            "        )\n",
            "        (bottleneck_channels): Sequential(\n",
            "          (0): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_block): Sequential(\n",
            "    (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (cls_layer): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): Flatten(start_dim=1, end_dim=-1)\n",
            "    (3): Linear(in_features=768, out_features=7000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({'epoch': 20, 'model_state_dict': model.state_dict(),\n",
        "            'criterion_state_dict': criterion.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "           'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'scaler_state_dict': scaler.state_dict()},\n",
        "           f'/content/drive/MyDrive/IDL_Code_Model/HW2P2_Model/early_submission_model_epoch_{epoch}.pth', _use_new_zipfile_serialization = False)"
      ],
      "metadata": {
        "id": "Jgwwctsk2G61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "WEIGHT_FILE = '/content/drive/MyDrive/IDL_Code_Model/HW2P2_Model/early_submission_model_epoch_19.pth'\n",
        "checkpoint = torch.load(WEIGHT_FILE, map_location=device)\n",
        "epoch = checkpoint['epoch']\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "criterion.load_state_dict(checkpoint['criterion_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "QhG869n447FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "EjvwFukfohT_",
        "outputId": "9408c97d-c175-4b80-fca5-73e4abbb06ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network(\n",
            "  (backbone): Sequential(\n",
            "    (0): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4))\n",
            "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
            "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ZeroPad2d(padding=(1, 0, 1, 0), value=0.0)\n",
            "    (13): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
            "    (17): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (cls_layer): Linear(in_features=512, out_features=7000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrChwbscbYkj",
        "outputId": "4c6709e0-b13b-474e-d8fa-f131f34ca1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20: Train Acc 0.0143%, Train Loss 8.8918, Learning Rate 0.0994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20: Train Acc 0.0258%, Train Loss 8.7391, Learning Rate 0.0976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20: Train Acc 0.0744%, Train Loss 8.5232, Learning Rate 0.0946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20: Train Acc 0.1846%, Train Loss 8.2660, Learning Rate 0.0905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20: Train Acc 0.3405%, Train Loss 7.9924, Learning Rate 0.0854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20: Train Acc 0.6510%, Train Loss 7.6986, Learning Rate 0.0794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20: Train Acc 1.0245%, Train Loss 7.3949, Learning Rate 0.0727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20: Train Acc 1.6641%, Train Loss 7.0899, Learning Rate 0.0655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20: Train Acc 2.7201%, Train Loss 6.7684, Learning Rate 0.0578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20: Train Acc 4.2039%, Train Loss 6.4400, Learning Rate 0.0500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20: Train Acc 6.1470%, Train Loss 6.1153, Learning Rate 0.0422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20: Train Acc 8.8284%, Train Loss 5.7913, Learning Rate 0.0345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20: Train Acc 11.9892%, Train Loss 5.4896, Learning Rate 0.0273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20: Train Acc 15.3775%, Train Loss 5.2066, Learning Rate 0.0206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20: Train Acc 19.0290%, Train Loss 4.9601, Learning Rate 0.0146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20: Train Acc 22.6319%, Train Loss 4.7504, Learning Rate 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20: Train Acc 26.1390%, Train Loss 4.5827, Learning Rate 0.0054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20: Train Acc 28.9406%, Train Loss 4.4564, Learning Rate 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20: Train Acc 30.9853%, Train Loss 4.3746, Learning Rate 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                                                                 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20: Train Acc 32.1057%, Train Loss 4.3363, Learning Rate 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    # You can add validation per-epoch here if you would like\n",
        "\n",
        "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        epochs,\n",
        "        100 * num_correct / (len(train_loader) * batch_size),\n",
        "        float(total_loss / len(train_loader)),\n",
        "        float(optimizer.param_groups[0]['lr'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKb2iD_9gdpX"
      },
      "source": [
        "# Classification Task: Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le1o-OVjfeN9",
        "outputId": "5454d04d-ee77-42d3-b1c0-e06559190346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 6.2571%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "num_correct = 0\n",
        "for i, (x, y) in enumerate(val_loader):\n",
        "\n",
        "    x = x.cuda()\n",
        "    y = y.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "\n",
        "    num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()\n",
        "print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf-RvdTyApRB",
        "outputId": "cfe61182-df4d-4911-edc3-ddbe5ff3fbdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08Zv2AWFrfVP"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestSet(Dataset):\n",
        "    # It's possible to load test set data using ImageFolder without making a custom class.\n",
        "    # See if you can think it through!\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td_qvGwr16z0"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                         drop_last=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f015a975-6926-4cd3-c80b-4a628d2eb28d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation: 6.2571%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "\n",
        "res = []\n",
        "for i, (x) in enumerate(test_loader):\n",
        "\n",
        "    # TODO: Finish predicting on the test set.\n",
        "    x = x.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x)\n",
        "\n",
        "    res.extend(torch.argmax(outputs, axis=1))\n",
        "\n",
        "    batch_bar.update()\n",
        "    \n",
        "batch_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_loader)"
      ],
      "metadata": {
        "id": "BGj_mwdKxvO0",
        "outputId": "3629010d-50bd-4069-b92b-a7a0b54d5fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(res)"
      ],
      "metadata": {
        "id": "URw-xfEzx711",
        "outputId": "18db11f0-cd67-4874-bc87-d2e6079f7954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35000"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxatBfT4jSQ",
        "outputId": "544c8d03-bfd0-44d7-cddc-82b6288adaf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "\r  0% 0.00/541k [00:00<?, ?B/s]\r100% 541k/541k [00:00<00:00, 2.67MB/s]\n",
            "Successfully submitted to Face Recognition"
          ]
        }
      ],
      "source": [
        "! kaggle competitions submit -c 11-785-s22-hw2p2-classification -f classification_early_submission.csv -m early"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n",
        "\n",
        "This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."
      ],
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls verification/verification/dev | wc -l\n",
        "!cat verification/verification/verification_dev.csv | wc -l\n",
        "!cat /content/verification_early_submission.csv | wc -l"
      ],
      "metadata": {
        "id": "ZV-WsTi9LrVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed850dc-c9ed-4423-89e4-5a4a0367146a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000\n",
            "166801\n",
            "667601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n",
        "        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # We return the image, as well as the path to that image (relative path)\n",
        "        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"
      ],
      "metadata": {
        "id": "m1YtIwxuL7H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98lmjm0S4tHR"
      },
      "outputs": [],
      "source": [
        "val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n",
        "                                       ttf.Compose(val_transforms))\n",
        "val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n",
        "                                             shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "# early_stop = 0\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    # print(len(path_names))\n",
        "    # early_stop += 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try the final outputs too!\n",
        "        feats = model(imgs, return_feats=True) \n",
        "    for emb_index in range(len(feats)):\n",
        "        feats_dict[path_names[emb_index]] = feats[emb_index]\n",
        "            \n",
        "        # print(feats_dict)\n",
        "    # if early_stop == 1:\n",
        "      # break\n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qw45H-eMyyn",
        "outputId": "70a1d691-8069-4726-ee96-0af1134112e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does this dict look like?\n",
        "print(list(feats_dict.items())[0])\n",
        "print(list(feats_dict.items())[0][1].shape)"
      ],
      "metadata": {
        "id": "k6TG6RD6NTtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0313a50c-311b-45aa-f4de-bb9bd866b97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('000b28b024.jpg', tensor([1.2569, 0.3534, 0.2190, 4.7730, 3.1352, 1.4041, 0.5100, 0.5319, 0.5085,\n",
            "        0.9756, 2.9520, 0.6323, 0.6236, 0.5534, 0.2738, 0.5251, 0.3794, 0.5342,\n",
            "        0.3272, 0.9717, 1.3553, 1.7167, 0.4047, 1.2829, 0.3706, 0.5238, 0.7662,\n",
            "        0.4161, 1.2471, 1.3157, 3.1988, 0.2249, 0.4256, 4.0080, 0.4246, 2.1858,\n",
            "        0.2173, 0.2536, 0.2647, 0.2485, 0.0968, 3.9496, 0.1020, 3.7258, 1.4275,\n",
            "        0.0650, 0.5717, 1.3966, 0.1236, 0.2548, 0.8670, 0.4743, 0.9593, 2.1185,\n",
            "        2.2810, 0.1006, 0.6388, 1.0772, 0.3143, 0.5435, 0.1444, 0.3195, 0.2811,\n",
            "        0.6508, 1.0987, 1.8814, 2.0824, 0.2072, 0.3854, 0.6865, 0.2229, 1.5995,\n",
            "        2.4710, 0.2762, 2.0633, 1.0991, 0.3682, 0.0956, 3.9837, 0.8518, 0.8782,\n",
            "        2.4152, 1.0134, 0.7126, 0.1568, 0.5427, 0.8678, 4.2020, 0.9462, 0.1952,\n",
            "        2.7003, 2.2646, 3.9707, 2.3978, 1.0811, 0.1821, 0.0913, 0.0999, 2.7244,\n",
            "        0.8473, 0.4098, 0.1846, 0.1424, 0.1086, 0.2833, 0.7411, 0.1076, 0.4617,\n",
            "        0.1577, 0.3351, 0.3372, 0.2561, 1.5713, 0.8890, 0.1568, 0.4889, 0.2067,\n",
            "        0.0448, 0.1579, 0.1270, 0.6089, 0.2165, 0.1872, 0.1607, 1.1284, 1.2645,\n",
            "        0.7125, 0.0423, 0.3965, 0.2185, 0.5875, 0.6077, 1.6863, 0.8369, 0.1813,\n",
            "        0.6715, 1.6645, 0.7078, 2.1482, 0.5866, 0.5345, 0.6991, 1.2667, 1.2863,\n",
            "        0.2465, 2.3525, 0.8321, 0.4508, 0.5623, 0.4118, 0.3623, 0.7736, 2.6826,\n",
            "        0.7493, 0.6142, 0.1691, 0.2505, 0.5761, 0.3859, 0.2154, 0.2842, 0.1763,\n",
            "        0.4071, 0.6678, 0.6144, 0.3453, 1.8286, 0.6987, 1.0716, 0.0391, 0.3252,\n",
            "        0.1568, 0.4183, 2.4204, 0.2030, 0.3768, 0.4715, 0.2551, 0.3943, 0.5239,\n",
            "        2.2783, 0.3306, 0.3474, 2.2358, 0.7853, 4.8688, 0.2614, 0.7353, 0.1289,\n",
            "        0.4580, 0.9691, 2.3360, 0.3489, 0.6515, 0.4304, 3.0222, 0.9759, 0.0438,\n",
            "        1.0243, 2.0829, 0.6748, 1.0922, 2.5802, 0.3890, 2.6016, 0.0714, 1.1521,\n",
            "        1.1364, 0.1373, 0.4551, 0.2937, 0.4953, 2.8676, 0.4879, 0.8946, 0.3822,\n",
            "        1.2737, 1.5750, 0.1982, 0.4129, 1.1630, 0.2657, 0.6768, 0.1540, 3.7774,\n",
            "        0.7682, 0.4255, 0.2807, 0.2592, 0.1217, 2.9648, 0.2441, 0.2235, 0.9375,\n",
            "        0.1392, 0.2956, 1.1916, 0.5584, 0.8931, 1.5854, 0.2137, 0.5302, 0.6164,\n",
            "        1.5008, 0.3692, 0.6535, 0.6581, 0.7186, 0.2278, 0.3280, 1.1242, 0.2598,\n",
            "        0.8511, 4.1671, 0.2918, 0.5807, 1.0289, 0.1047, 0.7414, 0.8231, 0.0846,\n",
            "        2.9449, 0.9104, 0.6256, 0.2459, 0.2432, 0.2409, 0.7743, 3.0048, 0.8801,\n",
            "        0.9487, 0.6942, 0.2841, 0.5886, 0.6864, 0.5276, 0.5930, 0.3876, 2.4809,\n",
            "        1.3921, 0.3710, 2.1627, 0.5836, 0.3974, 1.7847, 0.9851, 0.8752, 0.6606,\n",
            "        0.8144, 0.3607, 0.1696, 0.3410, 0.2991, 1.6357, 0.3822, 0.2875, 1.9094,\n",
            "        0.4485, 0.1232, 1.4337, 0.1039, 0.1227, 0.2921, 0.0830, 0.4693, 3.2673,\n",
            "        1.9808, 0.7443, 1.5212, 0.0594, 3.1785, 0.4021, 0.5695, 1.2183, 0.3716,\n",
            "        3.8357, 0.3185, 0.0708, 1.0437, 0.2588, 0.2902, 0.1255, 1.3834, 0.6063,\n",
            "        5.4269, 0.7872, 0.2209, 0.3367, 0.3209, 0.2118, 0.1559, 0.0679, 0.1833,\n",
            "        1.2091, 0.2044, 6.3543, 0.1851, 0.5284, 0.4172, 0.3150, 0.9987, 3.1575,\n",
            "        0.1393, 0.5896, 0.8836, 4.0558, 0.7509, 0.9525, 0.1609, 0.5515, 0.2565,\n",
            "        0.6839, 3.1549, 1.5295, 1.2357, 0.4819, 0.2760, 0.6518, 0.1074, 0.1598,\n",
            "        0.4579, 0.1158, 0.1580, 3.9077, 0.2459, 0.8151, 0.8243, 0.4046, 1.5541,\n",
            "        0.3730, 3.3538, 0.1878, 0.1932, 3.5257, 2.5827, 0.7817, 0.3210, 0.2636,\n",
            "        0.5676, 0.4060, 0.1429, 0.2634, 0.1830, 1.3479, 0.9487, 0.9292, 0.6038,\n",
            "        0.6134, 0.6607, 0.5404, 2.9641, 0.4857, 0.4638, 0.2774, 1.2322, 3.8947,\n",
            "        0.3692, 0.7917, 1.6970, 0.3789, 4.0683, 0.1970, 0.5513, 1.6501, 0.0753,\n",
            "        1.2429, 0.0983, 0.2403, 2.4395, 4.1139, 0.1819, 0.6683, 0.1833, 0.7034,\n",
            "        0.5052, 0.3743, 0.1205, 0.2679, 0.3811, 0.9571, 0.3749, 1.8654, 0.3966,\n",
            "        1.3913, 1.3422, 2.9313, 3.9938, 1.1598, 0.3660, 0.3380, 0.3744, 1.4756,\n",
            "        0.1283, 0.1684, 0.3920, 1.0456, 0.0450, 0.6386, 0.5533, 0.3944, 0.3666,\n",
            "        0.8242, 0.2747, 0.1402, 0.2111, 1.4406, 0.4148, 1.3837, 2.4830, 0.6501,\n",
            "        0.4602, 0.3277, 0.8278, 1.4169, 0.6814, 0.5892, 0.8366, 0.2647, 0.2137,\n",
            "        0.4682, 0.8634, 0.8494, 0.4908, 3.7794, 0.8441, 0.4337, 0.7390, 0.2750,\n",
            "        0.6555, 1.0787, 0.0710, 0.2514, 0.3666, 3.3264, 0.1321, 2.5336, 0.1189,\n",
            "        1.5945, 0.4406, 0.3609, 0.5693, 0.3212, 1.0530, 0.5723, 0.7807, 1.0967,\n",
            "        0.6772, 0.3419, 2.0519, 0.0815, 0.0897, 0.4026, 0.3784, 0.5549, 0.0963,\n",
            "        0.3321, 1.9679, 0.2345, 0.1170, 0.2819, 0.6594, 0.2396, 3.0778, 0.3858,\n",
            "        4.7361, 0.3905, 0.2250, 0.7105, 0.1228, 0.6172, 0.5236, 1.4993],\n",
            "       device='cuda:0'))\n",
            "torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input1 = torch.randn(2)\n",
        "print(input1)\n",
        "input2 = torch.randn(2)\n",
        "print(input2)\n",
        "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "output = cos(input1, input1)\n",
        "print(output.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyAhFgOyJCcR",
        "outputId": "ec925a62-eb9f-4031-bc06-06bd4312b3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.2630, -0.3452])\n",
            "tensor([-0.7791, -0.5361])\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "similarity_metric = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "gt_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2, gt = line.split(\",\")\n",
        "\n",
        "    # TODO: Use the similarity metric\n",
        "    # How to use these img_paths? What to do with the features?\n",
        "    # similarity = similarity_metric(...)\n",
        "    pred_similarities.append(similarity_metric(feats_dict[img_path1[4:]], feats_dict[img_path2[4:]]).item())\n",
        "    gt_similarities.append(int(gt))\n",
        "\n",
        "pred_similarities = np.array(pred_similarities)\n",
        "gt_similarities = np.array(gt_similarities)\n",
        "\n",
        "print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"
      ],
      "metadata": {
        "id": "_zuqds2qNO6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a4a9e1-e0dd-4882-caf0-d55dc10a0703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.7949669511548569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verification Task: Submit to Kaggle"
      ],
      "metadata": {
        "id": "sakRa8oZOlKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n",
        "                                        ttf.Compose(val_transforms))\n",
        "test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n",
        "                                              shuffle=False, num_workers=1)"
      ],
      "metadata": {
        "id": "oDK3knDcOrOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "feats_dict = dict()\n",
        "for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n",
        "    imgs = imgs.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Note that we return the feats here, not the final outputs\n",
        "        # Feel free to try to final outputs too!\n",
        "        feats = model(imgs, return_feats=True) \n",
        "    \n",
        "    # TODO: Now we have features and the image path names. What to do with them?\n",
        "    # Hint: use the feats_dict somehow.\n",
        "    for emb_index in range(len(feats)):\n",
        "        feats_dict[path_names[emb_index]] = feats[emb_index]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igeRT3WxOrB_",
        "outputId": "0ec182de-c9fc-4996-d639-e0c34786f78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hE9g8wkSTtw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use cosine similarity between feature embeddings.\n",
        "# TODO: Find the relevant function in pytorch and read its documentation.\n",
        "similarity_metric = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "\n",
        "val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n",
        "\n",
        "\n",
        "# Now, loop through the csv and compare each pair, getting the similarity between them\n",
        "pred_similarities = []\n",
        "for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n",
        "    img_path1, img_path2 = line.split(\",\")\n",
        "\n",
        "    pred_similarities.append(similarity_metric(feats_dict[img_path1[5:]], feats_dict[img_path2[5:]]).item())\n",
        "    # TODO: Finish up verification testing.\n",
        "    # How to use these img_paths? What to do with the features?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4OZL_FNOq1r",
        "outputId": "1637ff0d-e0c5-41a1-d61e-b0a13109f1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,match\\n\")\n",
        "    for i in range(len(pred_similarities)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"
      ],
      "metadata": {
        "id": "fYXiglWkPBDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5zB7P8O687N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d86b78-6ee6-43d3-ec0d-b20058bbf4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 16.4M/16.4M [00:03<00:00, 4.53MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f verification_early_submission.csv -m early_submission"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f verification_sample_submission.csv -m early"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55ePHvmaTPW2",
        "outputId": "83d1dd57-4e38-4ad4-e12c-b93ee1059d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 6.90M/6.90M [00:03<00:00, 1.82MB/s]\n",
            "Successfully submitted to Face Verification"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALiq9PTl7KwY"
      },
      "source": [
        "# Extras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuAsK_tKhzH9",
        "outputId": "48fcf464-aa49-4a8b-e387-932a80a74200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 24 10:34:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    65W / 149W |   1012MiB / 11441MiB |     89%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# If you keep re-initializing your model in Colab, can run out of GPU memory, need to restart.\n",
        "# These three lines can help that - run this before you re-initialize your model\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW2P2_Convnext.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}